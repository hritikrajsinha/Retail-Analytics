{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ccc6ac-84ed-4cde-b85c-821905f34675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import (\n",
    "    col, coalesce, lit, trim, regexp_replace, to_timestamp, try_to_timestamp, when, from_unixtime, current_timestamp,udf\n",
    ")\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "import re, dateutil.parser\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s-%(name)s-%(levelname)s-%(message)s')\n",
    "\n",
    "def safe_load_yaml(file_path):\n",
    "    try:\n",
    "        if not os.path.isfile(file_path):\n",
    "            logging.error(f\"Configuration file not found: {file_path}\")\n",
    "            raise FileNotFoundError(f\"Missing configuration file: {file_path}\")\n",
    "        else:\n",
    "            with open(file_path, 'r') as f:\n",
    "                return yaml.safe_load(f)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading YAML file: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_bronze_table(catalog, source_schema, source_table):\n",
    "    try:\n",
    "        df = spark.read.table(f\"{catalog}.{source_schema}.{source_table}\")\n",
    "        logging.info(f\"bronze table {source_table} loaded successfully into dataframe\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading bronze table: {e}\")\n",
    "\n",
    "def write_silver_table(df, catalog, target_schema, target_table):\n",
    "    try:\n",
    "        df.write.mode(\"overwrite\").insertInto(f\"{catalog}.{target_schema}.{target_table}\", overwrite=True)\n",
    "        logging.info(f\"silver table {target_table} written successfully\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error writing silver table {target_table}: {e}\")\n",
    "\n",
    "def _parse_dirty_date_py(s):\n",
    "    if s is None:\n",
    "        return None, \"missing\"\n",
    "    s0 = str(s).strip()\n",
    "    if s0 == \"\":\n",
    "        return None, \"missing\"\n",
    "\n",
    "    # strip surrounding quotes\n",
    "    s0 = re.sub(r\"^[\\\"']|[\\\"']$\", \"\", s0)\n",
    "\n",
    "    # epoch detection (10 or 13 digits)\n",
    "    digits = re.sub(r\"\\D\", \"\", s0)\n",
    "    try:\n",
    "        if re.fullmatch(r\"\\d{13}\", digits):\n",
    "            ts = int(digits) / 1000.0\n",
    "            dt = datetime.utcfromtimestamp(ts)\n",
    "            return dt.date(), \"epoch_ms\"\n",
    "        if re.fullmatch(r\"\\d{10}\", digits):\n",
    "            ts = int(digits)\n",
    "            dt = datetime.utcfromtimestamp(ts)\n",
    "            return dt.date(), \"epoch_s\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # normalize ISO-ish strings\n",
    "    s1 = re.sub(r\"Z$\", \"\", s0)          # strip trailing Z\n",
    "    s1 = s1.replace(\"T\", \" \")           # replace T with space\n",
    "    s1 = re.sub(r\"\\.\\d{1,6}\", \"\", s1)   # strip fractional seconds\n",
    "\n",
    "    try:\n",
    "        dt = dateutil.parser.parse(s1, yearfirst=True, dayfirst=False)\n",
    "        return dt.date(), \"parsed_yearfirst\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            dt = dateutil.parser.parse(s1, dayfirst=True, yearfirst=False)\n",
    "            return dt.date(), \"parsed_dayfirst\"\n",
    "        except Exception:\n",
    "            return None, \"unparsed\"\n",
    "\n",
    "# UDFs for Spark\n",
    "_parse_date_udf = udf(lambda s: _parse_dirty_date_py(s)[0], DateType())\n",
    "_parse_status_udf = udf(lambda s: _parse_dirty_date_py(s)[1], StringType())\n",
    "\n",
    "def normalize_date_column_safe(df, raw_col, out_col=\"OrderDate\", status_col=\"date_status\"):\n",
    "    df2 = df.withColumn(\"_raw_for_date\", trim(col(raw_col).cast(\"string\")))\n",
    "    df2 = df2.withColumn(out_col, _parse_date_udf(col(\"_raw_for_date\")))\n",
    "    df2 = df2.withColumn(status_col, _parse_status_udf(col(\"_raw_for_date\")))\n",
    "    return df2.drop(\"_raw_for_date\")\n",
    "\n",
    "\n",
    "# load configs\n",
    "global_config = safe_load_yaml('/Workspace/Users/hritikraj143@gmail.com/Retail-Analytics/Config/global_config.yaml')\n",
    "catalog = global_config['catalog']\n",
    "silver_config = safe_load_yaml('/Workspace/Users/hritikraj143@gmail.com/Retail-Analytics/Config/silver_config.yaml')\n",
    "source_schema = silver_config['silver']['sales_transformed']['source_schema']\n",
    "source_table = silver_config['silver']['sales_transformed']['source_table']\n",
    "target_schema = silver_config['silver']['sales_transformed']['target_schema']\n",
    "target_table = silver_config['silver']['sales_transformed']['target_table']\n",
    "logging.info('Configs loaded successfully')\n",
    "\n",
    "# load bronze table\n",
    "sales_landing = load_bronze_table(catalog, source_schema, source_table)\n",
    "\n",
    "# remove invalid or null orderid rows\n",
    "sales_landing = sales_landing.filter(col('OrderID').isNotNull())\n",
    "logging.info('OrderID: Nulls rows removed')\n",
    "sales_landing = sales_landing.withColumn('OrderID', sales_landing['OrderID'].cast('BIGINT'))\n",
    "logging.info('OrderID: typecasted to BIGINT')\n",
    "sales_landing = sales_landing.withColumn('CustomerID', coalesce(col('CustomerID'), lit('UNKNOWN_CUSTOMER')))\n",
    "logging.info('CustomerID: Nulls replaced with UNKNOWN_CUSTOMER')\n",
    "sales_landing = sales_landing.withColumn('ProductID', coalesce(col('ProductID'), lit('UNKNOWN_PRODUCT')))\n",
    "logging.info('ProductID: Nulls replaced with UNKNOWN_PRODUCT')\n",
    "sales_landing = normalize_date_column_safe(df=sales_landing, raw_col=\"Date\")\n",
    "cols_to_drop=[\"Date\",\"date_status\"]\n",
    "sales_landing=sales_landing.drop(*cols_to_drop)\n",
    "logging.info('Date: Date format cleaned and column name replaced to OrderDate')\n",
    "sales_landing=sales_landing.withColumn('Quantity', sales_landing['Quantity'].cast('FLOAT'))\n",
    "logging.info('Quantity: typecasted to FLOAT')\n",
    "sales_landing=sales_landing.withColumn('Quantity', sales_landing['Quantity'].cast('INT'))\n",
    "logging.info('Quantity: typecasted to INT')\n",
    "sales_landing=sales_landing.withColumn('Is_Quantity_Missing',when(col('Quantity').isNull(),lit(True)).otherwise(lit(False)))\n",
    "logging.info('New Column Added: Is_Quantity_Missing')\n",
    "sales_landing=sales_landing.withColumn('Quantity',coalesce(col('Quantity'), lit(0)))\n",
    "logging.info('Quantity: Nulls replaced with 0')\n",
    "sales_landing=sales_landing.withColumn('Price', regexp_replace(col('Price'),\"[^0-9.-]\", \"\"))\n",
    "logging.info('Price: strings removed from values')\n",
    "sales_landing=sales_landing.withColumn('Price', when(col('Price')==\"\",None).otherwise(col('Price').cast('DOUBLE')))\n",
    "logging.info('Price: typecasted to DOUBLE')\n",
    "sales_landing=sales_landing.withColumn('Is_Price_Missing',when(col('Price').isNull(),lit(True)).otherwise(lit(False)))\n",
    "logging.info('New Column Added: Is_Price_Missing')\n",
    "sales_transformed=sales_landing\n",
    "write_silver_table(sales_transformed, catalog, target_schema, target_table)\n",
    "# display(sales_transformed)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sales_transformed",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
